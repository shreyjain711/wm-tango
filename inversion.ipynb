{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3930b9f2-881a-4d82-8153-0e009b50ef31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'wm-tango' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/shreyjain711/wm-tango.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738ababf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/wm-tango\n",
      "/home/ec2-user/SageMaker/wm-tango\r\n"
     ]
    }
   ],
   "source": [
    "%cd '/home/ec2-user/SageMaker/wm-tango'\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e09d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21ccbd-eec9-4044-a13c-2f1fcc738746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools==70.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (70.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install setuptools==70.3.0\n",
    "!pip install -r requirements.txt --quiet\n",
    "!pip install jax==0.4.23 --quiet\n",
    "!pip install jaxlib==0.4.23 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "402efbd1-e6e6-44a8-9c7f-4f5913d8b92e",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Environmental Sounds\n",
    "# prompt_1 = \"Rain falling softly on a tin roof during a quiet night\"\n",
    "# prompt_2 = \"Waves crashing gently on a sandy beach\"\n",
    "\n",
    "# # Human Actions\n",
    "# prompt_3 = \"A crowd cheering and clapping at a concert\"\n",
    "# prompt_4 = \"Footsteps echoing in a large empty hallway\"\n",
    "\n",
    "# # Musical or Instrumental Sounds\n",
    "# prompt_5 = \"A piano playing a calm and soft melody in a quiet room\"\n",
    "# prompt_6 = \"An acoustic guitar being strummed near a campfire\"\n",
    "\n",
    "# # Animal Sounds\n",
    "# prompt_7 = \"Birds chirping in a dense forest at sunrise\"\n",
    "# prompt_8 = \"A dog barking excitedly in a backyard\"\n",
    "\n",
    "# # Urban and Cityscapes\n",
    "# prompt_9 = \"The hum of traffic on a busy city street\"\n",
    "# prompt_10 = \"Construction sounds with hammering and drilling\"\n",
    "\n",
    "\n",
    "\n",
    "# import IPython\n",
    "# import soundfile as sf\n",
    "# from tango import Tango\n",
    "\n",
    "# tango = Tango(\"declare-lab/tango2\")\n",
    "\n",
    "# p_num = 2\n",
    "# prompt = eval(f'prompt_{p_num}')\n",
    "# audio1 = tango.generate(prompt, wm_flag=True)\n",
    "# sf.write(f\"wm_p{p_num}.wav\", audio1, samplerate=16000)\n",
    "\n",
    "# audio2 = tango.generate(prompt, wm_flag=False)\n",
    "# sf.write(f\"no_wm_p{p_num}.wav\", audio2, samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13db2d5d-0e5a-4b4e-bab3-397065eab027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming necessary imports and initializations\n",
    "# batch_size = 1\n",
    "# audio_shape = (1, 1, 16000)  # Adjust based on TANGO's expected input shape\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Original noise\n",
    "# noise = torch.randn(batch_size, *audio_shape, device=device)\n",
    "\n",
    "# # Perform FFT\n",
    "# F_noise = torch.fft.fft(noise)\n",
    "\n",
    "# # Create a mask for the watermark (define create_watermark_mask accordingly)\n",
    "# def create_watermark_mask(shape):\n",
    "#     mask = torch.zeros(shape, dtype=torch.bool)\n",
    "#     # Example: Set mask for specific frequency bands\n",
    "#     mask[..., 100:200] = True\n",
    "#     return mask\n",
    "\n",
    "# mask = create_watermark_mask(F_noise.shape)\n",
    "\n",
    "# # Define the watermark pattern\n",
    "# watermark_pattern = torch.rand(mask.sum(), device=device)\n",
    "\n",
    "# # Embed the watermark\n",
    "# F_noise[mask] = watermark_pattern\n",
    "\n",
    "# # Inverse FFT to get the watermarked noise\n",
    "# watermarked_noise = torch.fft.ifft(F_noise).real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d650b7df-9d41-4774-b80f-87fbab05a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In TANGO's generation code\n",
    "# Original noise initialization\n",
    "# noise = torch.randn(batch_size, *audio_shape, device=device)\n",
    "\n",
    "# Use watermarked noise instead\n",
    "# noise = watermarked_noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a48f7b41-b026-45b9-a512-eb7cb86d57be",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007018327713012695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Fetching 9 files",
       "rate": null,
       "total": 9,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bba6e0b7024763944ddf26ad0e8dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/wm-tango/audioldm/audio/stft.py:42: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = pad_center(fft_window, filter_length)\n",
      "/home/ec2-user/SageMaker/wm-tango/audioldm/audio/stft.py:151: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=64, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_basis = librosa_mel_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet initialized randomly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/flan-t5-large were not used when initializing T5EncoderModel: ['decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_1.weight', 'lm_head.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.embed_tokens.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.17.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.2.DenseReluDense.wi_1.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint from: declare-lab/tango\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import snapshot_download\n",
    "from models import AudioDiffusion, DDPMScheduler\n",
    "from audioldm.audio.stft import TacotronSTFT\n",
    "from audioldm.variational_autoencoder import AutoencoderKL\n",
    "\n",
    "\n",
    "class AudioDiffusionInversion:\n",
    "    def __init__(self, name=\"declare-lab/tango\", device=\"cuda:0\"):\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        path = snapshot_download(repo_id=name)\n",
    "        \n",
    "        vae_config = json.load(open(\"{}/vae_config.json\".format(path)))\n",
    "        stft_config = json.load(open(\"{}/stft_config.json\".format(path)))\n",
    "        main_config = json.load(open(\"{}/main_config.json\".format(path)))\n",
    "        \n",
    "        self.vae = AutoencoderKL(**vae_config).to(device)\n",
    "        self.stft = TacotronSTFT(**stft_config).to(device)\n",
    "        self.model = AudioDiffusion(**main_config).to(device)\n",
    "        \n",
    "        vae_weights = torch.load(\"{}/pytorch_model_vae.bin\".format(path), map_location=device)\n",
    "        stft_weights = torch.load(\"{}/pytorch_model_stft.bin\".format(path), map_location=device)\n",
    "        main_weights = torch.load(\"{}/pytorch_model_main.bin\".format(path), map_location=device)\n",
    "        \n",
    "        self.vae.load_state_dict(vae_weights)\n",
    "        self.stft.load_state_dict(stft_weights)\n",
    "        self.model.load_state_dict(main_weights)\n",
    "\n",
    "        print (\"Successfully loaded checkpoint from:\", name)\n",
    "        \n",
    "        self.vae.eval()\n",
    "        self.stft.eval()\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.scheduler = DDPMScheduler.from_pretrained(main_config[\"scheduler_name\"], subfolder=\"scheduler\")\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_latents_from_audio(self, audio_waveform):\n",
    "        \"\"\"\n",
    "        Encodes audio into latents using STFT and VAE.\n",
    "        :param audio_waveform: Input waveform to encode.\n",
    "        :return: Latent representation of the audio.\n",
    "        \"\"\"\n",
    "        mel_spectrogram, _, _ = self.stft.mel_spectrogram(audio_waveform.unsqueeze(0).to(self.device))\n",
    "        latents = self.vae.encode_first_stage(mel_spectrogram.unsqueeze(0)).sample()\n",
    "        return latents\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @torch.no_grad()\n",
    "    def backward_diffusion(self, latents, num_inference_steps=50):\n",
    "        \"\"\"\n",
    "        Performs the backward diffusion process to reconstruct noise.\n",
    "\n",
    "        :param latents: Initial latent variables (e.g., generated from a VAE or forward diffusion process).\n",
    "        :param num_inference_steps: Number of diffusion steps to reverse the process.\n",
    "        :return: Reconstructed latents representing the noise.\n",
    "        \"\"\"\n",
    "        # Set up the scheduler for backward diffusion\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=latents.device)  # Use self.scheduler here\n",
    "        timesteps = self.scheduler.timesteps\n",
    "\n",
    "        # Start the reverse diffusion process\n",
    "        for t in tqdm(reversed(timesteps), desc=\"Reversing Diffusion\"):\n",
    "            # Current alpha value\n",
    "            alpha_prod_t = self.scheduler.alphas_cumprod[t]\n",
    "            beta_prod_t = 1 - alpha_prod_t\n",
    "\n",
    "            # Predict the noise added at this step\n",
    "            noise_pred = self.model.unet(\n",
    "                latents, t, encoder_hidden_states=None  # encoder_hidden_states=None for unconditional generation\n",
    "            ).sample\n",
    "\n",
    "            # Update the latents to the previous step\n",
    "            latents = (\n",
    "                (latents - beta_prod_t.sqrt() * noise_pred)  # Remove predicted noise\n",
    "                / alpha_prod_t.sqrt()  # Scale by alpha\n",
    "            )\n",
    "\n",
    "        return latents\n",
    "\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reconstruct_audio(self, latents):\n",
    "        \"\"\"\n",
    "        Reconstructs audio waveform from latents.\n",
    "        :param latents: Latent variables.\n",
    "        :return: Reconstructed audio waveform.\n",
    "        \"\"\"\n",
    "        mel_spectrogram = self.vae.decode_first_stage(latents)\n",
    "        audio_waveform = self.vae.decode_to_waveform(mel_spectrogram)\n",
    "        return audio_waveform\n",
    "\n",
    "    def invert(self, audio_waveform, num_inference_steps=50):\n",
    "        \"\"\"\n",
    "        Inverts an audio waveform through the diffusion pipeline.\n",
    "        :param audio_waveform: Input waveform to invert.\n",
    "        :param stft: STFT processor.\n",
    "        :param num_inference_steps: Number of diffusion steps.\n",
    "        :return: Reconstructed waveform and initial noise.\n",
    "        \"\"\"\n",
    "        latents = self.get_latents_from_audio(audio_waveform)#.transpose(-3, -2)\n",
    "        print(\"DBG\", latents.shape)\n",
    "        noise_latents = self.backward_diffusion(latents, num_inference_steps=num_inference_steps)\n",
    "#         reconstructed_waveform = self.reconstruct_audio(latents)\n",
    "        return noise_latents\n",
    "\n",
    "# Example Usage:\n",
    "# Initialize your model, scheduler, and STFT processor.\n",
    "# Initialize your model, scheduler, and STFT processor.\n",
    "# tango = Tango(name=\"declare-lab/tango\")\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "\n",
    "def load_audio(file_path):\n",
    "    \"\"\"Load audio file using librosa.\"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=None)  # Load audio with native sampling rate\n",
    "    return audio, sr\n",
    "\n",
    "inversion_pipeline = AudioDiffusionInversion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df7970-af94-4834-88a9-cd2a45ba582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sr = load_audio('generated_samples/wm_p1_n1_c7.wav') \n",
    "waveform = torch.tensor(waveform)\n",
    "noise = inversion_pipeline.invert(waveform, num_inference_steps=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef8b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e32d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
